{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjYUrnY5kbGITzJ/C5/F4T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khushboo1990-oss/1st-Mern-project/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZFpuUfNBIo0",
        "outputId": "89e96408-7bf3-4076-fceb-0a3c0505a874"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load the datasets\n",
        "X_train = pd.read_csv('/content/drive/MyDrive/X_Train_Data_Input.csv')\n",
        "Y_train = pd.read_csv('/content/drive/MyDrive/Y_Train_Data_Target.csv')\n",
        "X_test = pd.read_csv('/content/drive/MyDrive/X_Test_Data_Input.csv')\n",
        "Y_test = pd.read_csv('/content/drive/MyDrive/Y_Test_Data_Target.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ELwJkXjEYVZR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "7C_2Zxfi9mPH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the ID columns (assuming 'ID' is the first column)\n",
        "X_train.drop(columns=['ID','Column9'], inplace=True)\n",
        "Y_train.drop(columns=['ID'], inplace=True)\n",
        "X_test.drop(columns=['ID','Column9'], inplace=True)\n",
        "Y_test.drop(columns=['ID'], inplace=True)"
      ],
      "metadata": {
        "id": "Uu5ZJB6x9roo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select multiple columns for mode calculation\n",
        "mode_values = X_train[['Column0','Column4','Column3', 'Column14', 'Column15']].mode().iloc[0]  # Get mode for each column\n",
        "\n",
        "# Fill missing values with the mode for each corresponding column\n",
        "X_train[['Column0','Column4','Column3', 'Column14', 'Column15']] = X_train[['Column0','Column4','Column3', 'Column14', 'Column15']].fillna(mode_values)\n",
        "\n",
        " # Select multiple columns for mode calculation\n",
        "mode_values = X_test[['Column0','Column4','Column3', 'Column14', 'Column15']].mode().iloc[0]  # Get mode for each column\n",
        "\n",
        "# Fill missing values with the mode for each corresponding column\n",
        "X_test[['Column0','Column4','Column3', 'Column14', 'Column15']] = X_test[['Column0','Column4','Column3', 'Column14', 'Column15']].fillna(mode_values)"
      ],
      "metadata": {
        "id": "KJbki7qD9y5k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select multiple columns for median calculation\n",
        "median_values = X_train[['Column5', 'Column8']].median()  # Get median for each column\n",
        "\n",
        "# Fill missing values with the median for each corresponding column\n",
        "X_train[['Column5', 'Column8']] = X_train[['Column5', 'Column8']].fillna(median_values)\n",
        "\n",
        "# Select multiple columns for median calculation\n",
        "median_values = X_test[['Column5', 'Column8']].median()  # Get median for each column\n",
        "\n",
        "# Fill missing values with the median for each corresponding column\n",
        "X_test[['Column5', 'Column8']] = X_test[['Column5', 'Column8']].fillna(median_values)"
      ],
      "metadata": {
        "id": "p-c156n893-c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.fillna({'Column6': X_test['Column6'].mean()}, inplace=True)\n",
        "X_test.fillna({'Column6': X_test['Column6'].mean()}, inplace=True)"
      ],
      "metadata": {
        "id": "eXux_nHt97Si"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# Standard scale 'Column1' after handling non-numeric data\n",
        "X_train['Column1'] = scaler.fit_transform(X_train[['Column1']])\n",
        "X_test['Column1'] = scaler.fit_transform(X_test[['Column1']])\n",
        "X_train['Column2'] = scaler.fit_transform(X_train[['Column2']])\n",
        "X_test['Column2'] = scaler.fit_transform(X_test[['Column2']])"
      ],
      "metadata": {
        "id": "1OzXyDDt-Bhm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.drop(columns=['Column0','Column10','Column11','Column13','Column3','Column21'], inplace=True)\n"
      ],
      "metadata": {
        "id": "sMlhDLqZ-J8f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.drop(columns=['Column0','Column10','Column11','Column13','Column3','Column21'], inplace=True)"
      ],
      "metadata": {
        "id": "uz9WlLdIAzGa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9eF4mljPgLWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.drop('Column16', axis=1, inplace=True)\n",
        "X_test.drop('Column16', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "6G5B3DxTe1J5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.columns)\n",
        "print(X_test.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOmRAFBbfiCQ",
        "outputId": "9dd6255a-130c-419d-8c69-cf69e3418593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Column1', 'Column2', 'Column4', 'Column5', 'Column6', 'Column7',\n",
            "       'Column8', 'Column12', 'Column14', 'Column15', 'Column17', 'Column18',\n",
            "       'Column19', 'Column20'],\n",
            "      dtype='object')\n",
            "Index(['Column1', 'Column2', 'Column4', 'Column5', 'Column6', 'Column7',\n",
            "       'Column8', 'Column12', 'Column14', 'Column15', 'Column17', 'Column18',\n",
            "       'Column19', 'Column20'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnYX2RHvCBBT",
        "outputId": "6d8aa4e4-98be-4e65-d26c-e7bbaaf9555e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column1     0\n",
            "Column2     0\n",
            "Column4     0\n",
            "Column5     0\n",
            "Column6     0\n",
            "Column7     0\n",
            "Column8     0\n",
            "Column12    0\n",
            "Column14    0\n",
            "Column15    0\n",
            "Column17    0\n",
            "Column18    0\n",
            "Column19    0\n",
            "Column20    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test.isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv_-Ob0uDA1L",
        "outputId": "f3420029-2fec-4253-d54a-d1587d7ebbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column1     0\n",
            "Column2     0\n",
            "Column4     0\n",
            "Column5     0\n",
            "Column6     0\n",
            "Column7     0\n",
            "Column8     0\n",
            "Column12    0\n",
            "Column14    0\n",
            "Column15    0\n",
            "Column17    0\n",
            "Column18    0\n",
            "Column19    0\n",
            "Column20    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Convert the dataset into LightGBM dataset format\n",
        "train_data = lgb.Dataset(X_train, label=Y_train)\n",
        "test_data = lgb.Dataset(X_test, label=Y_test, reference=train_data)\n",
        "\n",
        "# Define the LightGBM parameters\n",
        "params = {\n",
        "    'objective': 'binary',  # for binary classification\n",
        "    'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\n",
        "    'metric': 'binary_logloss',  # Evaluation metric\n",
        "    'num_leaves': 31,  # Number of leaves in one tree\n",
        "    'learning_rate': 0.05,  # Step size for shrinking the contribution of each tree\n",
        "    'feature_fraction': 0.9  # Fraction of features to be used\n",
        "}\n",
        "\n",
        "# Train the model with early stopping using callbacks\n",
        "gbm = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    num_boost_round=100,\n",
        "    valid_sets=[test_data],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_prob = gbm.predict(X_test, num_iteration=gbm.best_iteration)  # Probability estimates\n",
        "y_pred = [1 if x > 0.5 else 0 for x in y_pred_prob]  # Convert probabilities to binary predictions\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HMCAxxldRKV",
        "outputId": "cde995a6-3fcb-4004-8886-cd2e1983d11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n",
            "Training until validation scores don't improve for 10 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's binary_logloss: 0.0548728\n",
            "Accuracy: 97.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(Y_test, Y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arhr-SwzeKdM",
        "outputId": "13bbc976-efdc-4abb-f047-20d741bf226a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99    237034\n",
            "           1       0.82      0.93      0.87     24678\n",
            "\n",
            "    accuracy                           0.97    261712\n",
            "   macro avg       0.91      0.95      0.93    261712\n",
            "weighted avg       0.98      0.97      0.98    261712\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Predictions\n",
        "Y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPhsBEv4fF6q",
        "outputId": "c1713fce-c275-40ec-e76b-73cdc7434e8e"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9727372073118542\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98    237034\n",
            "           1       0.83      0.89      0.86     24678\n",
            "\n",
            "    accuracy                           0.97    261712\n",
            "   macro avg       0.91      0.93      0.92    261712\n",
            "weighted avg       0.97      0.97      0.97    261712\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import precision_score, accuracy_score\n",
        "\n",
        "# Convert the dataset into LightGBM Dataset format\n",
        "train_data_selected = lgb.Dataset(X_train, label=Y_train)\n",
        "test_data_selected = lgb.Dataset(X_test, label=Y_test, reference=train_data_selected)\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'metric': 'binary_logloss',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.01,\n",
        "    'lambda_l1': 0.2,\n",
        "    'lambda_l2': 0.2,\n",
        "    'feature_fraction': 0.8,\n",
        "    'min_data_in_leaf': 50,\n",
        "}\n",
        "\n",
        "# Cross-validation with Stratified K-Folds\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Cross-validation precision evaluation\n",
        "model = lgb.LGBMClassifier(**params)\n",
        "precision_scores = cross_val_score(model, X_train, Y_train.values.ravel(), cv=kf, scoring='precision')  # or Y_train.to_numpy().ravel()\n",
        "print(f'Mean Precision (Cross-validated): {precision_scores.mean():.4f}')\n",
        "\n",
        "\n",
        "# Train the final model on the entire training set\n",
        "final_model = lgb.LGBMClassifier(**params)\n",
        "final_model.fit(X_train, Y_train)\n",
        "\n",
        "# Predict probabilities for test set\n",
        "y_pred_prob = final_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Adjust the decision threshold to increase precision\n",
        "threshold = 0.6  # You can experiment with this threshold to get better precision\n",
        "y_pred = [1 if prob > threshold else 0 for prob in y_pred_prob]\n",
        "\n",
        "# Calculate precision and accuracy\n",
        "precision = precision_score(Y_test, y_pred)\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "8VCd5C3ej7V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy_score(Y_test, y_pred))\n",
        "print(classification_report(Y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxgy5RpriPaz",
        "outputId": "aa21df3c-f598-4f35-e273-5572aef8c371"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9247799107415785\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96    237034\n",
            "           1       0.96      0.21      0.34     24678\n",
            "\n",
            "    accuracy                           0.92    261712\n",
            "   macro avg       0.94      0.60      0.65    261712\n",
            "weighted avg       0.93      0.92      0.90    261712\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Convert the dataset into LightGBM Dataset format\n",
        "train_data_selected = lgb.Dataset(X_train, label=Y_train)\n",
        "test_data_selected = lgb.Dataset(X_test, label=Y_test, reference=train_data_selected)\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'metric': 'binary_logloss',\n",
        "    'num_leaves': 30,\n",
        "    'learning_rate': 0.01,\n",
        "    'lambda_l1': 0.15,\n",
        "    'lambda_l2': 0.2,\n",
        "    'feature_fraction': 0.8,\n",
        "    'min_data_in_leaf': 40,\n",
        "    'scale_pos_weight':4, # Adjust this to handle class imbalance\n",
        "    'verbosity':1\n",
        "}\n",
        "\n",
        "# Cross-validation with Stratified K-Folds\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Cross-validation precision and recall evaluation\n",
        "model = lgb.LGBMClassifier(**params)\n",
        "\n",
        "# Using cross_val_score for both precision and recall\n",
        "precision_scores = cross_val_score(model, X_train, Y_train.values.ravel(), cv=kf, scoring='precision')\n",
        "recall_scores = cross_val_score(model, X_train, Y_train.values.ravel(), cv=kf, scoring='recall')\n",
        "\n",
        "print(f'Mean Precision (Cross-validated): {precision_scores.mean():.4f}')\n",
        "print(f'Mean Recall (Cross-validated): {recall_scores.mean():.4f}')\n",
        "\n",
        "# Train the final model on the entire training set\n",
        "final_model = lgb.LGBMClassifier(**params)\n",
        "final_model.fit(X_train, Y_train)\n",
        "\n",
        "# Predict probabilities for test set\n",
        "y_pred_prob = final_model.predict_proba(X_test)[:, 1]\n",
        "# Fix the threshold to a specific value (e.g., 0.5)\n",
        "threshold = 0.55  # Set threshold value here\n",
        "y_pred = [1 if prob > threshold else 0 for prob in y_pred_prob]\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(Y_test, y_pred)\n",
        "recall = recall_score(Y_test, y_pred)\n",
        "f1 = f1_score(Y_test, y_pred)\n",
        "\n",
        "print(f'Threshold: {threshold:.1f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3_w8zqTpe3C",
        "outputId": "d4a21d6b-50c4-4af9-a607-adcbf3f88501"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Info] Number of positive: 59226, number of negative: 568880\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170580 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2188\n",
            "[LightGBM] [Info] Number of data points in the train set: 628106, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094293 -> initscore=-2.262309\n",
            "[LightGBM] [Info] Start training from score -2.262309\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Info] Number of positive: 59226, number of negative: 568880\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104868 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2186\n",
            "[LightGBM] [Info] Number of data points in the train set: 628106, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094293 -> initscore=-2.262309\n",
            "[LightGBM] [Info] Start training from score -2.262309\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Info] Number of positive: 59226, number of negative: 568880\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184323 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2189\n",
            "[LightGBM] [Info] Number of data points in the train set: 628106, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094293 -> initscore=-2.262309\n",
            "[LightGBM] [Info] Start training from score -2.262309\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Info] Number of positive: 59227, number of negative: 568880\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074015 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2189\n",
            "[LightGBM] [Info] Number of data points in the train set: 628107, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094294 -> initscore=-2.262292\n",
            "[LightGBM] [Info] Start training from score -2.262292\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Info] Number of positive: 59227, number of negative: 568880\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072738 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2190\n",
            "[LightGBM] [Info] Number of data points in the train set: 628107, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094294 -> initscore=-2.262292\n",
            "[LightGBM] [Info] Start training from score -2.262292\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Info] Number of positive: 59226, number of negative: 568880\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074543 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2188\n",
            "[LightGBM] [Info] Number of data points in the train set: 628106, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094293 -> initscore=-2.262309\n",
            "[LightGBM] [Info] Start training from score -2.262309\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Info] Number of positive: 59226, number of negative: 568880\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073635 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2186\n",
            "[LightGBM] [Info] Number of data points in the train set: 628106, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094293 -> initscore=-2.262309\n",
            "[LightGBM] [Info] Start training from score -2.262309\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Info] Number of positive: 59226, number of negative: 568880\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073789 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2189\n",
            "[LightGBM] [Info] Number of data points in the train set: 628106, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094293 -> initscore=-2.262309\n",
            "[LightGBM] [Info] Start training from score -2.262309\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Info] Number of positive: 59227, number of negative: 568880\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113395 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2189\n",
            "[LightGBM] [Info] Number of data points in the train set: 628107, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094294 -> initscore=-2.262292\n",
            "[LightGBM] [Info] Start training from score -2.262292\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Info] Number of positive: 59227, number of negative: 568880\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074239 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2190\n",
            "[LightGBM] [Info] Number of data points in the train set: 628107, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094294 -> initscore=-2.262292\n",
            "[LightGBM] [Info] Start training from score -2.262292\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "Mean Precision (Cross-validated): 0.8214\n",
            "Mean Recall (Cross-validated): 0.9610\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "[LightGBM] [Info] Number of positive: 74033, number of negative: 711100\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138621 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2193\n",
            "[LightGBM] [Info] Number of data points in the train set: 785133, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094294 -> initscore=-2.262302\n",
            "[LightGBM] [Info] Start training from score -2.262302\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.2\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15\n",
            "Threshold: 0.6, Precision: 0.8548, Recall: 0.8893, F1 Score: 0.8717\n",
            "Accuracy: 0.9753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(Y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULjQDyV99K6h",
        "outputId": "550f8e11-2ee1-43be-cd74-02cb34104bdf"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99    237034\n",
            "           1       0.85      0.89      0.87     24678\n",
            "\n",
            "    accuracy                           0.98    261712\n",
            "   macro avg       0.92      0.94      0.93    261712\n",
            "weighted avg       0.98      0.98      0.98    261712\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DHX0pLNk4p-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(Y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWrBvOZEqQ34",
        "outputId": "cf2704bd-7807-47b1-f197-99dfddd07102"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98    237034\n",
            "           1       0.90      0.72      0.80     24678\n",
            "\n",
            "    accuracy                           0.97    261712\n",
            "   macro avg       0.94      0.86      0.89    261712\n",
            "weighted avg       0.96      0.97      0.96    261712\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Validate the model using cross-validation\n",
        "scores = cross_val_score(model, X_train, Y_train, cv=5, scoring='accuracy')\n",
        "print(f'Mean cross-validated accuracy: {scores.mean():.4f}')"
      ],
      "metadata": {
        "id": "zxJiF_XK-RBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Define the model\n",
        "model = LGBMClassifier()\n",
        "\n",
        "param_dist = {\n",
        "    'num_leaves': [31, 50, 70],\n",
        "    'learning_rate': [0.01, 0.1, 0.05],\n",
        "    'lambda_l1': [0.0, 0.1, 0.2],\n",
        "    'lambda_l2': [0.0, 0.1, 0.2],\n",
        "    'feature_fraction': [0.8, 0.9, 1.0],\n",
        "    'min_data_in_leaf': [20, 50, 100]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=50, cv=5, scoring='accuracy')\n",
        "random_search.fit(X_train, Y_train)\n",
        "\n",
        "print(f'Best Parameters: {random_search.best_params_}')\n"
      ],
      "metadata": {
        "id": "i3lMa1pqFmsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import precision_score, accuracy_score\n",
        "\n",
        "# Define parameters for LightGBM\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'metric': 'binary_logloss',\n",
        "    'num_leaves': 20,  # Reducing complexity\n",
        "    'learning_rate': 0.05,  # Trying a slightly higher learning rate\n",
        "    'lambda_l1': 0.2,\n",
        "    'lambda_l2': 0.2,\n",
        "    'feature_fraction': 0.8,\n",
        "    'min_data_in_leaf': 100,  # Reducing overfitting\n",
        "    'scale_pos_weight': 10,  # Adjusted for class imbalance\n",
        "    # 'early_stopping_round': 30,  # Shortened early stopping to avoid overfitting\n",
        "    'verbose': -1\n",
        "}\n",
        "\n",
        "# Prepare LightGBM dataset for training\n",
        "train_data_selected = lgb.Dataset(X_train, label=Y_train)\n",
        "valid_data_selected = lgb.Dataset(X_test, label=Y_test)\n",
        "\n",
        "# Train LightGBM model with early stopping\n",
        "gbm = lgb.train(params,\n",
        "                train_data_selected,\n",
        "                num_boost_round=1000,\n",
        "                valid_sets=[valid_data_selected],\n",
        "                valid_names=['validation'],\n",
        "                callbacks=[lgb.early_stopping(stopping_rounds=30)])\n",
        "\n",
        "# Predict probabilities for test set\n",
        "y_pred_prob = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
        "\n",
        "# Adjust the decision threshold to increase precision\n",
        "threshold = 0.7  # Increasing the threshold to reduce false positives\n",
        "y_pred = [1 if prob > threshold else 0 for prob in y_pred_prob]\n",
        "\n",
        "# Calculate precision and accuracy on the test set\n",
        "precision = precision_score(Y_test, y_pred)\n",
        "accuracy = accuracy_score(Y_test, y_pred)\n",
        "\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# # Feature importance (optional)\n",
        "# importance_df = pd.DataFrame({\n",
        "#     'Feature': X_train.columns,\n",
        "#     'Importance': gbm.feature_importance(importance_type='gain')\n",
        "# })\n",
        "# print(importance_df.sort_values(by='Importance', ascending=False))\n",
        "\n",
        "# Cross-validation precision evaluation\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "model = lgb.LGBMClassifier(**params)\n",
        "\n",
        "# Evaluate cross-validated precision\n",
        "precision_scores = cross_val_score(model, X_train, Y_train.values.ravel(), cv=kf, scoring='precision')\n",
        "print(f'Mean Precision (Cross-validated): {precision_scores.mean():.4f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7fEVNXflQd1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(Y_test, Y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQDwl1_LllTS",
        "outputId": "5aea393a-3a4a-48a5-8214-7f4f94ab686d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99    237034\n",
            "           1       0.82      0.93      0.87     24678\n",
            "\n",
            "    accuracy                           0.97    261712\n",
            "   macro avg       0.91      0.95      0.93    261712\n",
            "weighted avg       0.98      0.97      0.98    261712\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Y_train to 1D array if it's a single column DataFrame\n",
        "precision_scores = cross_val_score(model, X_train, Y_train.values.ravel(), cv=kf, scoring='precision')\n",
        "\n",
        "print(f'Mean Precision (Cross-validated): {precision_scores.mean():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sOrnUCuZoh8",
        "outputId": "e3e28f92-24c0-429b-c349-bf385b5924f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Precision (Cross-validated): 0.7590\n"
          ]
        }
      ]
    }
  ]
}